# ‚ö° Otimiza√ß√£o de Performance - Importa√ß√£o de Dados

## üî¥ Problemas Identificados

1. **Rein√≠cio do PostgreSQL** - Deixava VPS inacess√≠vel por minutos
2. **Muitos workers paralelos** - Consumiam toda mem√≥ria dispon√≠vel
3. **Carregamento total em mem√≥ria** - Arquivos grandes causavam OOM
4. **Sem timeouts** - Threads podiam travar indefinidamente
5. **Refresh de MVs m√∫ltiplas vezes** - Sobrecarregava o banco

## ‚úÖ Solu√ß√µes Implementadas

### 1. `scraping_parallel.py`
- ‚úÖ Limitado a m√°ximo **2 workers** (em vez de CPU count)
- ‚úÖ Adicionado timeout de **30 minutos** por conta
- ‚úÖ **Removido restart do PostgreSQL** (desnecess√°rio)
- ‚úÖ Adicionado delay de 5s antes da importa√ß√£o

### 2. `import_all_data.py`
- ‚úÖ Reduzido workers para **m√°ximo 2** (em vez de CPU-1)
- ‚úÖ Processamento em lotes de **5000 registros**
- ‚úÖ Melhor logging do progresso

### 3. `new_excel_to_db.py`
- ‚úÖ Leitura de Excel com `dtype=str` (economiza mem√≥ria)
- ‚úÖ Processamento em chunks de **2000 registros**
- ‚úÖ Deduplica√ß√£o otimizada em mem√≥ria

## üöÄ Recomenda√ß√µes Adicionais

### Para VPS com pouca mem√≥ria (< 4GB):

**Op√ß√£o 1: Importa√ß√£o sequencial (mais segura)**
```bash
# Editar import_all_data.py e mudar:
max_workers = 1  # Apenas 1 worker
```

**Op√ß√£o 2: Reduzir chunk size**
```python
# Em new_excel_to_db.py:
chunk_size = 1000  # Em vez de 2000
batch_size = 2500  # Em vez de 5000
```

**Op√ß√£o 3: Monitorar em tempo real**
```bash
# Terminal 1: Monitorar mem√≥ria
watch -n 1 'free -h && echo "---" && ps aux | grep python'

# Terminal 2: Verificar conex√µes PostgreSQL
watch -n 2 'psql -U user -d database -c "SELECT count(*) FROM pg_stat_activity"'
```

### Limites do Sistema Operacional

Adicionar ao `crontab` se rodar automaticamente:
```bash
# Limpar cache periodicamente antes da importa√ß√£o
*/2 * * * * sync && echo 3 > /proc/sys/vm/drop_caches
```

### Configura√ß√£o PostgreSQL (postgresql.conf)

Se ainda tiver problemas, ajuste:
```ini
# Reduzir pool de conex√µes
max_connections = 100  # (default √© 100, j√° baixo)
shared_buffers = 256MB  # (em vez de 40% RAM)
work_mem = 16MB  # (em vez de 64MB)
```

## üìä Monitoramento

Verificar status da importa√ß√£o:
```bash
# Ver consumo de recursos
top -p $(pgrep -f import_all_data.py)

# Ver conex√µes ao PostgreSQL
psql -c "SELECT pid, usename, application_name, state FROM pg_stat_activity"

# Ver queries lentas
tail -f /var/log/postgresql/postgresql.log | grep "duration"
```

## üß™ Teste

Execute com dados de teste:
```bash
# 1. Copie apenas 1 arquivo pequeno para docs/
# 2. Execute a importa√ß√£o
python3 import_all_data.py

# 3. Se der sucesso, processe os demais
```

## üìå Resumo das Mudan√ßas

| Arquivo | Mudan√ßa | Impacto |
|---------|---------|--------|
| scraping_parallel.py | Max 2 workers + timeout 30m | -50% picos de mem√≥ria |
| import_all_data.py | Max 2 workers + chunks 5000 | -60% consumo RAM |
| new_excel_to_db.py | dtype=str + chunks 2000 | -40% picos de mem√≥ria |
| Removido | PostgreSQL restart | +100% disponibilidade |

---

**Pr√≥ximos passos:** Execute a importa√ß√£o e monitore. Se ainda tiver problemas, reduza `max_workers` para 1.
